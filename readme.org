* cutslib
Utility library for working with ACT cuts. As I get more familiar with
ACT cuts, this library may significantly change. Note that i have only
tested these on tiger.

Some notations that i used in these codes:
- Routines: main building blocks of cuts pipeline
- Modules: main building blocks of postprocessing pipeline
- Recipes: quick command line tools for various routine work

** Installation
#+BEGIN_SRC bash
pip install -e .
#+END_SRC

** Dependency
Most of the dependency libraries should automatically install, with a few
exceptions that require manual installation
- =moby2=

** Recipes
Located in =cuts.recipes=. Notable mentions (such that i don't forget
them myself):
- =cuts.recipes.results.combine=:
  reduce the output of mpi-based cuts pipeline of a given version (i.e. cutparams_v3.par):
  #+BEGIN_SRC bash
  cuts results combine cutparams_v3.par
  #+END_SRC
  This will find the corresponding run directory, combine mpi sub-tasks and remove redundent files
- =cuts.recipes.results.promote=:
  promote a given cuts parameter to a new version:
  #+BEGIN_SRC bash
  cuts results promote cutparams_v3.par
  #+END_SRC
  This will automatically replace of all the version numbers inside the file.
- =cuts.recipes.run.submit=:
  submit jobs (on tiger) for cuts pipeline with a given cutparam file (i.e. cutparam_v3.par)
  #+BEGIN_SRC bash
  cuts run submit cutparam_v3.par
  #+END_SRC
- =cuts.recipes.run.list=:
  list all cutparams in all tags and their corresponding slurm jobs, tod completion status, very useful
  #+BEGIN_SRC bash
  cuts run list
  #+END_SRC
  To see more than the latest version use =cuts run list all=
- =cuts.recipes.run.errors=:
  list all the errors generated during the run with a given cutparam file (i.e. cutparam_v3.par)
  #+BEGIN_SRC bash
  cuts run errors cutparam_v3.par
  #+END_SRC
- =cuts.recipes.run.errors_tod=:
  grep errors during the run of a given cutparam file (i.e. cutparams_v0.par)
  #+BEGIN_SRC bash
  cuts run errors_tod cutparams_v0.par notfound > skip.txt
  #+END_SRC
- =cuts.recipes.update.submit=:
  submit jobs (on tiger) for updating cut crits with a given cutparam file (i.e. cutparam_v3.par)
  #+BEGIN_SRC bash
  cuts update submit cutparam_v3.par
  #+END_SRC
- =cuts.recipes.dets.get_ff_unstable=:
  get unstable detectors from the flatfield file in a given cutparam file (i.e.cutparams_v3.par)
  #+BEGIN_SRC bash
  cuts dets get_ff_unstable cutparams_v3.par
  #+END_SRC
- =cuts.recipes.dets.get_exclude=:
  same as above but to get the excluded detectors
  #+BEGIN_SRC bash
  cuts dets get_exclude cutparams_v3.par
  #+END_SRC
- =cuts.recipes.dets.union=:
  get the union of det_uid found in each of the input file
  #+BEGIN_SRC bash
  cuts dets union exclude.dict lowgain.dict unstable.dict
  #+END_SRC
- =cuts.recipes.dets.intersection=:
  same as above but to find the intersection instead of the union
  #+BEGIN_SRC bash
  cuts dets intersection exclude.dict unstable.dict
  #+END_SRC
- =cuts.recipes.release.tags=:
  generate release json file for selected list of cutparams
  #+BEGIN_SRC bash
  cuts release tags *s17*
  #+END_SRC
- =cuts.recipes.params.quickfix=:
  quickly fix cutparams in the command-line
  #+BEGIN_SRC bash
  cuts params quickfix forceNoResp False cutParams_v3.par > cutParams_v4.par
  #+END_SRC
  or for inplace fix
  #+BEGIN_SRC bash
  cuts params quickfix forceNoResp False cutParams_v3.par -i
  #+END_SRC
** Modules:
Notable mentions
- =collect_crit=:
  collect the cuts crit for all the TODs processed and generate a
  pickle file for easier processing
- =plot_cuts_thresholds=:
  take the pickle file generated by =collect_crit= and plot histograms
  of each of the cuts crit
- =get_flatfield=:
  generate atmospheric flatfield based on the atmosphere gain
- =plot_ff=:
  plot the flatfields (both planet and atmospheric)
- =plot_ff_binned=:
  same as above but for each pwv bin
- =plot_hits=:
  produce a hits map for the input TODs list
- =plot_array=:
  plot selected cuts crits (mean) on an array plot
- =plot_killed_array=:
  plot number of detectors alive after each cut crit
- =plot_ld_loading=:
  plot number of live detectors as a function of optical loading
- =plot_live_fraction=:
  plot the fraction that a detector is cut on an array plot
- =plot_planet_cal=:
  plot planet calibration as a function of optical loading
- =plot_resp_hist=:
  plot the histogram of number of detectors with valid responsivity
  from bias-step measurements.
- =plot_rms_gain=:
  produce a 2D histogram of responsivity versus planet flatfield gain
- =plot_season_stats=:
  produce season based statistics such as live dets per pwv, histograms + scatter plots of cuts parameters
- =report=:
  generate a pdf report summarizing the latest run. It needs
  additional dependencies such as emacs and latex.
- =debug_patho=:
  pathology debugger (with ipdb inside)
- =export_json=:
  export the pathologies of each TOD into json format for easy visualization
  using this [[https://github.com/guanyilun/tod_viz][web-based tool]].
- =todlist_for_map=:
  generate the list of TODs that is available for mapping
- =build_todinfo=:
  generate =todinfo.hdf= file for =enki=
- =create_filedb=:
  generate =filedb.txt= for =enki= based on a given cut release
- =create_todinfo=:
  generate =todinfo.txt= for =enki= based on a given cut release
- =generate_h5=:
  convert cut crit pickle file to hdf5 format
- =init_cutparam=:
  generate boilerplate cutparams based on templates
- =match_bs_tod=:
  match bias step files to each tod
- =merge_for_cmb=:
  merge all cuts for cmb
- =plot_waterfall=:
  produce time space and frequency space waterfall plots for list of tods
- =select_good_tods=:
  identify list of tods good for atmosphere flatfielding.
- =update_cuts=:
  update cuts crit or excluded detector list without re-runing the entire cut-pipeline
** Q&A
*** 1. How is this different from moby2 cuts?
Most of the relevant scripts for cuts in moby2 have been migrated here. The purpose is
such that i can manage them easily without having to worry about compiling moby2, etc.
*** 2. What has been migrated from moby2 specifically?
- =moby2.analysis.tod_ana.pathologies= -> =cutslib.pathologies=
- =moby2.scripting.pathologies_tools= -> =cutslib.pathologies_tools=
- =moby2.analysis.tod_ana.visual= -> =cutslib.visual=
- =bin/{get,process}_cuts= -> various routines
*** 3. How to make =report.py= module work? What does it depend on externally?
It generates pdf report during the post-processing of the cuts results. I implemented
it using org format as it's more lucid than tex. The org document is converted to pdf
using emacs. This means that you will need to have two things available: 1. latex:
it is by default available on =tiger=, but it's lacking some of the libraries for the
pdf to compile properly so you will have to make these libraries available somehow.
The two libraries that i found missing are =ulem= and =wrapfig=. What i did was to
download these packages manually and place them in =~/texmf/tex/latex/ulem= and
=~/texmf/tex/latex/wrapfig= respectively. Emacs is not by default available on tiger
but you can easily load it as a module with =module load emacs= in your =.bashrc=.
Then you should have everything you need to get report generated
*** 4. How does recipe work?
Recipe as in my notation is simply a binding from command-line tool to a function in
the library. This is so that i can easily manage large number of cuts related command-line
tools by categories and have them easily accessible with the =cuts= keyword. These recipes
are defined in =cutslib.recipes=. An example collection of recipes is
=cutslib.recipes.results=. It contains some functions that help me manage the outputs from
cuts pipeline. For example, there is a function to merge mpi sub-task output into a single
one called =combine(cpar)=, where =cpar= refers to the path to a given cutparams file
(i.e. cutparams_v0.par). This function is made directly accessible in the command-line
via
#+BEGIN_SRC
cuts results combine cutparams_v0.par
#+END_SRC
Note that my convention is that each recipe function returns a list of commands to be
executed in sequence. It can be useful in some occasions.
*** 5. Environment variables, how are they used?
The environment variables can be used to define where the cuts depot is or where the
working directory of the cuts is, etc. The default values and the keys are described
in =cutslib.environ=. These can be set in the =.bashrc=. Here is how i setup
the environment variables
#+BEGIN_SRC bash
export CUTS_DEPOT="/projects/ACT/yilung/depot/"
export CUTS_DIR="/projects/ACT/yilung/cuts/"
export CUTS_PYENV="myenv"
export CUTS_SHARED_DEPOT="/projects/ACT/yilung/actpol_data_shared/"
export CUTS_MAPDATA="/scratch/gpfs/yilung/mapdata"
#+END_SRC
Note that by default i am assuming you are running on a local
environment given by =CUTS_PYENV=.  This is so that the python
environment can be set properly before submitting slurm jobs.  To
furthur elaborate, =CUTS_DEPOT= is where the outputs of the cuts
are. =CUTS_DIR= is where the working directory of the cuts are. This
is such that various recipes such as =cuts run list= can find the
parameter files that i am working on, it's a pre-requisite for the
emacs cut plug-in. =CUTS_PYENV= is the local python environment as
described earlier. =CUTS_SHARED_DEPOT= is the path to shared depot,
and =CUTS_MAPDATA= is the path to the directory to work with mapdata.
This is used by the =build_todinfo.py= module.
*** 6. How did i exclude detectors with unstable flatfield and low gain detector?
As low gain detectors tend to result in low rms detectors which gets a
large weight in the map-making process, they are often identified
beforehand and excluded. Also, as the unstable detectors in the
flatfield are by default not removed in the cuts pipeline, they will
need to be excluded by hand. This question is to help me remember what
i need to do to get both of them excluded. First, to get the detectors
with unstable flatfield,
#+BEGIN_SRC bash
cuts dets get_ff_unstable cutparams_v3.par > unstable.dict
#+END_SRC
where cutparams_v3.par is an example cut parameter file of
interests. To get the detectors with low gain (from flatfield), what i
find easist is to run =export_json= module with =limit=1= option to
generate a json file associated with one of the TODs in the list, and
then pass this file to the TOD_viz visualizer (linked below).  In the
visualizer, i will manually en-circle the group of outliers with
abnormally low gain (in ff plot) using the interactive selection
tool. The det_uids of the selected detectors will be printed in the
browser console (pressing Ctrl+Shift+I to see it).  Suppose that i
store the resulting list of detectors into a file called
=lowgain.dict=. The last step is merge the =unstable.dict= and
=lowgain.dict= into the existing excluded list. I will do
#+BEGIN_SRC bash
cuts dets get_exclude cutparams_v3.par > exclude.dict
cuts dets union exclude.dict unstable.dict lowgain.dict > exclude_v2.dict
#+END_SRC
Then I will update the cutParam file to use =exclude_v2.dict= as the
excluded detector list. Note that if this is the only change, one does
not need to re-run cuts pipeline entirely because exclude detector list
is applied in the very end. One can remove the db file
generated in the run_* folder and re-run. This will skip the glitch,
planet, source cuts and multi-frequency analysis which will save
considerable amount of time. One can also run
#+BEGIN_SRC bash
cuts submit update_crit cutparams_v0.par
#+END_SRC
which will be even faster.
*** 7. How to use =cuts.recipes.params.quickfix= to change some parameters?
Before i forget, this is how to perform a quick command-line fix of cutparams
#+BEGIN_SRC bash
cuts params quickfix exclude \"exclude_v2.dict\" cutParams_v3.par > cutParams_v4.par
#+END_SRC
Note the escaped string here. For non-string expression,
#+BEGIN_SRC bash
cuts params quickfix forceNoResp False cutParams_v3.par > cutParams_v4.par
#+END_SRC
To edit in place, add =-i= at the end of statement
#+BEGIN_SRC bash
cuts params quickfix forceNoResp False cutParams_v3.par -i
#+END_SRC
*** 8. What's my typical workflow when running cuts?
This is to document my workflow in case i forget them:
- switch to the right python environment (for my case =myenv=)
- launch emacs with =emacs -nw=
- launch the cuts-run plugin in emacs with =F7=
To edit an existing cut version:
- press =o= / =O= to open the cutparams/cutParams file for editing
To create a new version of cut:
- press =P= to promote the version of cut and edit like above
After editing:
- press =S= to submit the job to slurm
- press =L= to monitor the logs
When the job is done:
- press =[= to check the run folder
- press =E= to check the error logs to makes sure no unexpected errors occurred
- press =c= to reduce the mpi sub-tasks to a combined result
Post-processing
- press =f= to load up post-processing script
- select post-processing modules to run with =c-c m=
- press =F= to run the post-processing pipeline
- rsync the report to a local machine to view the reports
*** 9. What's my typical workflow when evaluating cuts?
Here are what Loic suggested: 1. look for outlying statistics in the
pathological parameters, if they don't represent a significant portion
of the data, one can remove them. 2. Look at the cut parameters on the
array to identify systematic effects. 3. Look at the number of dets
cut by each crit, usually =norm=, =gain=, and =rms= cut are most
stringent, if that's not the case it's worth investigating, it could
mean the crit is applied to stictly. 4. Look at the Uranus calibration
plot to evaluate the performance of the gain and flatfield. If the scatter
is about 3-4%, it's very good.

Here are what i found informative to do in addition to what's
above: Load the season stats with =cutslib.SeasonStats=.
#+BEGIN_SRC python
from cutslib import SeasonStats
ss = SeasonStats(tag='pa4_f150_s17_c11_v0', use_theta2=True)
#+END_SRC
The option =use_theta2= represents converting the =corr= parameter which
roughly stands for the cosine angle between a det with the common mode, into
the angle^2 (theta2). I found this to be a nicer variable to look at. Make a
histogram of cut parameters with the thresholds (not guidline) shown.
#+BEGIN_SRC python
ss.hist()
#+END_SRC
Look for signs of thresholds falling in the middle of a very smooth distribution.
This could be a sign of a threshod being too strict. Look at the pair-wise histogram
triangular plot with
#+BEGIN_SRC python
ss.tri()
# or with density plot instead of histogram plot
# ss.tri(density=True)
#+END_SRC
This is often useful to show clusters of dets that have different statistics. Look for these
clusters and inspect them with, for example,
#+BEGIN_SRC python
ss.find_matches(ss.gain<0.6, ss.rms<1.5, alone=True)
#+END_SRC
This will show you what these dets are in row/col space and if they correlate with pwv, etc.
If some stats fall out of lower bounds in the low pwv, it could mean the lower bound is not
set properly. I expect a systematic problem to be pwv independent, and perhaps have a unique
pattern in row/col space, look for these patterns with the =find_matches= functions. The
thresholds can be dynamically changed via
#+BEGIN_SRC python
ss.update_style({'normLive':{'crit':[200,300000]},'gainLive':{'crit':[0.7, 5]}})
ss.update_critsel()
#+END_SRC
Then i can look at the histograms / tri plots again to see the effects of these new thresholds.
Another very useful thing to look at is the number of dets cut by each criteria at various pwv.
This can be plotted with
#+BEGIN_SRC python
ss.view_cuts()
#+END_SRC
It shows which cut is most stringent at various ranges of pwv. I often
identify the most stringent cuts at low pwv and ask myself whether
that's necessarily bad or just very good weather without a good
atmosphere common mode. If one wants to save some dets at low pwv only,
one can do something like
#+BEGIN_SRC python
ss.find_match(ss.gain<0.6, ss.rms<1.6, ss.pwv<0.6, alone=True)
ss.sel2hdf(ss.sel, 'recover.hdf')
#+END_SRC
Then one can include this hdf to the cut parameters to save these dets.
*** 10. How did i generate the lists of tods for preliminary studies?
From s17 onwards, we genenerate a list of 1000 TODs for preliminary studies before running on the full season. To generate the list of tods, i used the binary script =select_tod_adv= in the bin folder. It takes in a parameter file such as =tod.par= in the template folder. For example, in the bin folder,
#+BEGIN_SRC bash
./select_tod_adv ../templates/tod.par
#+END_SRC
The parameters in =tod.par= is explained in the commends in =select_tod_adv= script.

*** 11. How did i prepare the cuts h5 file for machine learning studies?
After the main cuts pipeline and post-processing pipeline finish, run
the =generate_h5= post-processing module to create the h5 file for
=mlpipe= to use.  The model generated from =mlpipe= can in turn be
used to generate the det-level cuts using =model_to_cuts=
post-processing module. Note that this is not the final version of
cuts that can be used in production as it only contains the det-level
results. To generate full results, one needs to run a modified cuts
pipeline that will merge in all other cuts including partial cuts,
etc. The modules required to do this is written in
=cutslib.thirdparty= and the binary code that executes it is named
=update_cuts= in bin folder.

*** 12. What's the main changes in the base-line cuts in s17 onwards compared with before?
1. calibration is purely based on planet flatfield and bias-step responsivity. No atmosphere gain is applied anymore. This is achieved by including the freq in the noAtmFreq in the cutParams file.
2. detectors without valid calibrations are no longer included. As we now calibrate by planet flatfield and bias-step responsivity, dets without flatfield or responsivity cannot be included reliably.
3. scan chunk level cuts are turned off by default as we no longer expect our detectors to behave drastically different in the duration of 10 mins.
4. pre-selection is no longer used to generate the cuts but instead used to calculate the common modes. The cuts are purely determined by the cuts thresholds, modulos manually exclusion.

*** 13. How does release work
Before i forget, this is how to generate a cut release. First go to the root cutparam directory. Suppose i want to release the latest cuts for s17, simply run
#+BEGIN_SRC bash
cuts release tags *s17*
#+END_SRC
It will promote me for a version name, author name and a file to write to. The default values should be reasonable for most of the cases. Here is a sample output, including the prompts:
#+begin_example
Version: (default: 20200328)
Author: (default: Yilun Guan)
{
  "version": "20200328",
  "date": "Mar 28, 2020",
  "tags": {
    "pa4_f150_s17_c11": {
      "params": "/projects/ACT/yilung/cuts/pa4_f150_s17_c11/cutparams_v6.par",
      "tag_out": "pa4_f150_s17_c11_v6",
      "tag_cal": "pa4_f150_s17_c11_v6",
      "tag_partial": "pa4_f150_s17_c11_v3_partial",
      "tag_planet": "pa4_f150_s17_c11_v3_planet",
      "tag_source": "pa4_f150_s17_c11_v3_source"
    },
    "pa4_f220_s17_c11": {
      "params": "/projects/ACT/yilung/cuts/pa4_f220_s17_c11/cutparams_v3.par",
      "tag_out": "pa4_f220_s17_c11_v3",
      "tag_cal": "pa4_f220_s17_c11_v3",
      "tag_partial": "pa4_f220_s17_c11_v2_partial",
      "tag_planet": "pa4_f220_s17_c11_v2_planet",
      "tag_source": "pa4_f220_s17_c11_v2_source"
    }
  },
  "author": "Yilun Guan",
  "depot": "/projects/ACT/yilung/depot/"
}
Write to: (default: /projects/ACT/yilung/depot/release_20200328.txt)
#+end_example
This is written in json format which should be easily digestible by
modules in the downstream. An example module that read the release is
the =create_todinfo= module which generates the enki compatible
=todinfo.txt= file. After the release json file is generated, tag
the =cutparam= repo with the same tag to keep track of the parameters
used in the given tag.

*** 14. How to run final processing pipeline
What i call final processing pipeline is practically the same as the
normal post-processing pipeline except that it doesn't depend on a
specific cut version. It is supposed to depend on all cuts version. An
example of this is to create the =todinfo.hdf= file for map-making
which relies on information from all arrays / freqs pairs. An example
of such module is the =create_todinfo= module in
=cutslib.recipes.create_todinfo=. Here is an example final-processing
configuration script.
#+BEGIN_SRC
# Final Post-processing pipeline
# =====================================
# Full pipeline assuming all tag-specific processing is accomplished

[pipeline]
pipeline = create_todinfo

[create_todinfo]
cut_release = 20200327
obs_details_cmb = wide_01h_n
obs_details_noncmb = uranus
#+END_SRC
The only difference is that it doesn't depend on the cutparams, except
that everything should be the same as normal post-processing
pipelines.
*** 15. How to setup environment for enki
To setup environment to run enki or enki related modules do
#+BEGIN_SRC
module load enki
module load so_stack
#+END_SRC
on tiger.
*** 16. What are different types of modules?
At the moment, the post-processing pipeline (=cutspipe=) supports
different types of modules. By default the module is an =internal=
type which means it will be loaded from =cutslib.modules= by
default. The default name is the pipeline module name unless specified
otherwise by =module= option. It also supports external module that
can be loaded from a given file. To use this one needs to specify
=type= option and =file= option to tell the pipeline where to look for
the module. Here is an example configuration for an external module
#+BEGIN_SRC
[create_todinfo]
type = external
file = /home/yilung/work/cutslib/cutslib/modules/create_todinfo.py

cut_release = 20200327
obs_details_cmb = wide_01h_n
obs_details_noncmb = uranus
outfile = /scratch/gpfs/yilung/mapdata/s17_subset/todinfo.txt
#+END_SRC
Another type of module that is supported is the =command= type. It
is essentially like running command in bash, here is an example
#+BEGIN_SRC
[test_module]
type = command
dir = pa4_f220_s18_c11
command = ls ${dir}
#+END_SRC
Note that we have enabled and used the =ExtendedInterpolation= syntax
for variable substitution. Refer to =ConfigParser= for more
documentation on how to use this.
*** 17. How to use =cutspipe= modules with mpi?
The post-processing pipeline =cutspipe= is made to support mpi. In
order for a module to make use of mpi, it needs to be supported in the
module explicitly. The relevant dependencies are injected through the
argument to the =run(p)= method call with the rank, size and comm
contained in =p.rank=, =p.size=, =p.comm=. If a module support mpi, it
can declared in the option with =mpi=True=. This allows the main
program to pass mpi control to the module instead of forcing it to run
with =rank=0= for other modules that do not support mpi. See
=cutslib.modules.collect_crit= module for an example of how mpi is
supported.
*** 18. How do i remove tod runs specific errors from running next time?
Sometimes errors are unavoidable. For example, preselection errors are unavoidable
sometimes and this happens at the very end of the pipeline so if we know for sure a
tod is going to fail pre-selection, we shoudn't have spent long time to run the previous
routines. One way out of this is to include these tods in a skip list. For example,
here is how i get the tods with an error message containing "Presel" (preselection errors)
and pipe the results to a skip list. Then i can put this list in the cutparams file
in the reject_depot list.
#+BEGIN_SRC bash
cuts run errors_tod cutparams_v0.par presel > skip_tods.txt
#+END_SRC
*** 19. How to add or recover detector cuts for different TODs?
In the cutParams file that is an option called =include_cuts=, which belongs to
=pathologyParams= section and look like
#+BEGIN_SRC python
     'include_cuts'         : [
                            {
                               "type": "exclude",
                               "file": "/projects/ACT/yilung/cuts/pa4_f150_s17_c11/cuts_de_ge500_norm_ge2e4_rms_le1.5.hdf",
                            },
                            {
                               "type": "exclude",
                               "file": "/projects/ACT/yilung/cuts/pa4_f150_s17_c11/cuts_gain_le0.2_rms_le1.5.hdf",
                            },
     ]
#+END_SRC
There are two types of operations supported at this moment: =exclude=
and =recover=. =exclude= means to remove dets from some TODs, and
=recover= means to prevent these list of dets being cut for some
TODs. A good use case of this is to recover some of the detectors cut
in low pwv TODs. The file format supported is an hdf file that is
generated with the name of tod as key and a boolean mask representing
dets selection. This file can be easily generated using =cutslib.season.SeasonStats=
object.
*** 20. Quickly update crit or dets cuts without rerunning =findPathologies=
I have mentioned using =cuts submit update_crit= to update the cuts criteria
without rerunning the entire pathology finding pipeline. This is useful when
one wants to submit jobs to slurm and don't mind the queuing process, if a quick
look is needed, there is a module doing similar thing in =cutslib.modules.update_cuts=.
It is mpi-enabled so running through 1000 TODs takes about a minute on one node.
One just need to add to the configuration file
#+BEGIN_SRC
[update_cuts]
mpi=True
#+END_SRC
and include in the =pipeline= section. Run it like =mpirun -n 40 cutspipe post.ini=.
It's good to remove the previous TODCuts, calibration from depot, though
i haven't encountered a case where not doing so causes noticable issues. It's good
to run this in a debug note because this is very quick to run, here is the command
to launch a debug node with 40 tasks required
#+BEGIN_SRC
salloc -pdebug --nodes 1 --ntasks-per-node=40 --time=1:00:00
#+END_SRC
*** 21. How to setup enki for a given cut release?
Here is what i do:

- create an =.enkirc= in the home directory with, for example,
#+BEGIN_SRC
root = '/scratch/gpfs/yilung/mapdata'
#+END_SRC
- make sure environment variable =CUTS_MAPDATA= points to the same path specified
in =.enkirc=. Also specify the path to the cuts depot in =CUTS_DEPOT= environment
variable. For example,
#+BEGIN_SRC bash
export CUTS_DEPOT="/projects/ACT/yilung/depot/"
export CUTS_MAPDATA="/scratch/gpfs/yilung/mapdata"
#+END_SRC
- Build metadata for a given release. Use the following cutspipe modules
#+BEGIN_SRC
[create_todinfo]
cut_release = 20200327
obs_details_cmb = wide_01h_n
obs_details_noncmb = uranus
dataset = s17_subset
outfile = ${cuts_mapdata}/${dataset}/todinfo.txt

[create_filedb]
cut_release = 20200327
dataset = s17_subset
template = default
outfile = ${cuts_mapdata}/${dataset}/filedb.txt

[build_todinfo]
n_tasks = 10
dataset = s17_subset
sel = s17
#+END_SRC
The fields should be self explanary. These modules produce =todinfo.txt=, =filedb.txt=
and =todinfo.hdf= based on a given =cut_release= tag. with this we are all set to run
enki. Some of existing mapping routines in cutslib include =map_tods= which produces
binned map of TODs, and =tod2map2= which produce maximum likelihood code using =tod2map2=
in =tenki=. here are the default configurations for them, one only needs to overwrite
relevant fields when running.
#+BEGIN_SRC
[map_tods]
type=command
ntasks=10
module_load=enki so_stack
area=wide_01h_n
dataset=s17_subset
sel=s17,cmb
tag=test
nrandom=100
mapdata=${cuts_mapdata}
command=mpirun -n {ntasks} python map_tod.py ${mapdata}/area/${area}.fits \
        "${sel}" ${CUTS_MAPDATA}/{dataset}/{tag} --dataset ${dataset} \
        --nrandom ${nrandom}

[tod2map2]
type = command
module_load = enki so_stack
ntasks = 4
area = wide_01h_n
sel = s17,cmb
dataset = s17_subset
tag = test
downsample = 4
verbosity = 2
nomp = 4
mapdata = ${cuts_mapdata}
command = srun -n ${ntasks} enki tod2map2 -S sky:${area} "${sel}" ${mapdata}/${tag} --verbosity=${verbosity} --downsample=${downsample} --dataset ${dataset}
#+END_SRC
more relevant mapping script written by Adri that's relevant for cuts
can be found in cuts_validation repo linked in the useful link section.
*** 22. Why is gain calculation in moby2 so confusing?
The manipulation of gains in moby2 / loic's codes are beyond
confusing. I kept getting confused on the calculation and every time
needing to spend lots of time recapping what's done. Let me write down
what i understand now before i forget again...
- gains are calculated with SVD using calibrated TODs (with input
  flatfield and resp) as part of =lowFreqAnal= [[https://github.com/ACTCollaboration/moby2/blob/master/python/analysis/tod_ana/pathologies.py#L1883][here]] for each frequency
  window.
- the gains for each frequency window are then normalized by mean of a
  set of "reliable" detectors. This set of dets is determined by the
  preselection, that is, dets that are preselected in more than half
  of the frequency windows. See [[https://github.com/ACTCollaboration/moby2/blob/master/python/analysis/tod_ana/pathologies.py#L1691][this line]].
- estimate detector gains using the mean of the gains measured from
  different frequency windows, this is similar to how other low
  frequency pathological parameters are derived. See [[https://github.com/ACTCollaboration/moby2/blob/master/python/analysis/tod_ana/pathologies.py#L1700][this line]].
- The gains estimated above is then divided by the input flatfield to
  "remove" the effects of flatfield. In the case that the input
  flatfield 100% accurate for a TOD, the gain without removing
  flatfield will be exactly 1, so after removing the flatfield the
  gain will be 1/ff which matches with the gains from the input
  flatfield. See [[https://github.com/ACTCollaboration/moby2/blob/master/python/analysis/tod_ana/pathologies.py#L416][this line]].
- This gains will be stored in the crit field in the pathologies
  object and saved to disk.
- When cuts are to be generated based on the pathologies object, that
  is, in the =recoverScanCuts= method of a =reportPathologies= object,
  it calls the =calibrateValues= method in the =makeNewSelections=
  method of pathology object. Here the flatfield effects are added
  back again to the gain (=gain <- gain * ff=). See [[https://github.com/ACTCollaboration/moby2/blob/master/python/analysis/tod_ana/pathologies.py#L695][this line]]. This
  gain is normalized once again in the =normalizeGains= function by
  the mean of the gain from a good subset of the detectors which is
  found after outlier removal. See [[https://github.com/ACTCollaboration/moby2/blob/master/python/analysis/tod_ana/pathologies.py#L2266][this line]]. It is this set of gains
  that the cut criteria specified in the cut parameter scripts apply
  to.
- On the other hand, the gain stored in the pathology object is later
  collected into a pickle file by =collectSeasonCrit= script and
  stored under =gainLive= in the pickle file. Note that these gains
  have flatfield removed. See [[https://github.com/ACTCollaboration/moby2/blob/master/bin/collectSeasonCrit][this script]]. Atmosphere flatfields are
  generated based on these =gainLive= field as =mean(1/gain)= using
  a subset of dets after outlier rejection. See [[https://github.com/ACTCollaboration/moby2/blob/master/bin/generateFlatfield#L32][this line]]. These gains
  are normalized again by the mean of this subset of dets which is also
  labelled as stable. This process of taking =mean(1/gain)= and normalization
  is repeated for a total of *10 times*... The results of which, after some
  threshold-based cuts become the output atmosphere flatfield.

This is the life story of the gains in =moby2=. To make things less
(maybe more) confusing, some of these scripts are changed in cutslib,
in particular, the =collectSeasonCrit= script has been superseded by the
=collect_crit= module, apart from adding lots of new fields to the pickle
file, i changed it such that the gain stored in this pickle file is the
same as what the cuts crit, that is, =calibrateValues= is called before
collecting. =generateFlatfield= script is now superseded by the
=get_flatfield= module and the =plot_ff= module, where i manually
remove the input flatfield to be comparable with the input flatfield.

*** 23. How to pre-process the bias-step measurements such that they match with TODs?
BS measurements provided by Patty is aranged in ctimes, i will first
need to match them to TODs. To do this matching for a given season and
array, first make sure that the bias-step files from Patty is rsync-ed
to the depot with an example structure like this (for pa4), notice
this should be the same as the file-structure used by Patty.
#+begin_example
/home/yilung/work/depot/biasstep/2019/calibration_20200328/pa4
#+end_example
After re-assuring that the bias-steps files are rsync-ed completely,
one can then use the module =match_bs_tod= to match them to tods with
a configuration file like this:
#+BEGIN_SRC
[pipeline]
cutparam = cutparams_v0.par
output_dir = /projects/ACT/yilung/depot/Postprocess/
pipeline = match_bs_tod
[match_bs_tod]
tag = calibration_20200328
#+END_SRC
i store it in =pre.ini= and run =cutspipe pre.ini= to complete the
step. It should take one or two minutes to complete the matching. Note
that this only needs to be done once per array. When this is done, one
can supply it to the cutParams config with something like
#+begin_example
'config'   : [
{
   "type": "depot_cal",
   "tag": "pa6_s18_bs_calibration_20200325",
   "depot": '/projects/ACT/yilung/depot',
   "name": "biasstep"
}],
#+end_example
Substitute the season, array and bs tag accordingly.
*** 24. What does cuts status mean?
One of the cuts products is a so-called selectTODs file that contains
a list of TODs each with a so-called "cuts status". It can have three
values: 0, 1, 2. A quick way to remember what it means is to always
use those with cuts status of 2. This value is calculated in a
two-step process. First, all TODs that have been processed in the cuts
pipeline that have not caused any program failures will get one score
(cuts status += 1). Then, the TODs will be screened using with a few
criteria that usually include a cut on PWV (i.e. <3) and a cut on the
number of detectors alive (i.e. ld>150), and a cut on ctime range to
exclude specific time periods (i.e. based on ObsTimes supplied in the
act shared depot). TODs that pass this set of criteria get another
score (cuts status += 1). At the end, we want the TODs with full score
(cuts status = 2) for mapping.
*** 25. What are the major steps in the cuts pipeline?
1. First, a list of TODs is generated from the observation catalog
   that contains names of TODs to generate cuts for.
2. Load TODs and check the length of TOD, if it's too short (below a specified threshol) then the TOD is removed
3. Fix MCE frame error
4. Generate source cuts
   1. source cuts can be generated based on a pre-existing catalog
   2. new idea is to use a sky mask derived from cut map
   3. fill the source cuts
5. Generate planet cuts, and fill the cuts
6. Generate glitch cuts
7. Analyze scan and flag turnarounds
8. Calibrate TODs with bias-steps and planet flatfield
9. Perform multi-frequency analysis
   1. Define various frequency windows including 10 low-frequency windows,
      a mid-frequency window, and a high-frequency windows
   2. In low frequency windows, we expect our signal to be dominated by
      atmosphere which becomes a common mode for all detectors.
      1. The full correlation matrix between detectors is computed
      2. A pre-selection is performed based on the correlation matrix
         to identify a "majority" group of detectors that are well
         correlated.
      3. The pre-selected detectors are used to extract the common mode.
      4. We then evaluate each detector in terms of how well it correlates
         with the common mode. Three pathological parameters are computed
         for evaluation named as gain, corr, and norm.
      5. This step is repeated for each of the 10 low frequency windows.
      6. A mean of different windows is taken for each of the parameters.
   3. In the high-frequency window, we expect our signal to be white
      noise dominated. Here we evaluate detectors based on their noise
      levels detectors and also deviation from a white (gaussian)
      statistics. Three pathological parameters are computed for evaluation
      named as rms, skew, and kurt.
   4. The mid-frequency window is defined to hopefully capture the thermal
      1/f when the atmosphere 1/f turns off and before the white noise dominates.
      Two pathological parameters are defined in this window named as DE and MFE
      standing for drift error and mid-frequency error.
   5. Finally a pathological parameter called jump is calculated that quantify
      how much jump a detector sees.
   6. At the end of the multi-frequency analysis, a total of 9
      pathological parameters are collected for each detector for each TOD. These
      are stored into the depot for furthur study
10. Generate cuts
    1. Based on predefined thresholds for each of the 9 pathological parameters and cut
       detectors with outlying statistics
    2. Cut detectors pre-supplied in a exlusion list. This is often a
       list of known unstable detector based on inputs from previous
       pipeline.
    3. Merge in time-domain cuts like glitch cuts and turnaround cuts.
    4. Detectors with a large fraction (above a predefined threshold, usually 40%)
       of samples cut is removed entirely.
    5. Detectors affected by more than a predefined number of glitches
       (usually 50000) are cut.
    6. Detectors with missing calibrations are cut
    7. Optionally exclude / recover per detector cuts for different
       TODs based on external inputs. This is for performing surgical
       operations to improve cuts quality.
11. Generate calibration based on the product of flatfield and
    bias-steps and write to disk.
12. This marks the end of normal cuts pipeline. What follows is the
    post-processing pipeline which will be described next.
*** 26. What are the main steps in post-processing pipeline?
As of 200801, a full post-processing pipeline involves running the
following modules in sequence:
1. collect_crit
2. select_good_tods
3. plot_cuts_thresholds
4. plot_array
5. plot_killed_array
6. get_flatfield
7. plot_ff
8. todlist_for_map
9. plot_planet_cal
10. plot_ld_loading
11. plot_live_fraction
12. plot_rms_gain
13. plot_season_stats
14. report
15. export_json
Refer to [[*Modules:][Modules]] section for detailed descriptions for each
module. This produces all relevant products and debugging report
etc. for furthur evaluation.
*** 27. What's the meaning of cuts tags?
Cuts are often tagged like =pa4_f150_s17_c11_v0=. The first three
fields are array, frequency band, and season code respectively. The
fourth field refers to the cuts software version. It tells you which
software is running to generate this given set of cuts. For the main
cuts pipeline will all start with a letter =c= followed by a version
number. There may be other specially taylored cuts generated for other
purposes, the difference will be shown in this field. For example, a
special study of cosmic ray hits required me to generate cuts with no
buffer, these cuts are tagged as =cr= in this field. The last field
(=v0=) tracks the changes in the parameters used in generating cuts.
Sometimes there is an additional field denoting the types of cuts a tag
contains. For example, =pa4_f150_s17_c11_partial=. This indicates that
the cuts under this given tag contain the partial cuts (glitch cuts) only.
Here's a summary of some of the common names used in this field

|---------+------------------------|
| name    | purpose                |
|---------+------------------------|
| partial | glitch cuts only       |
| source  | point source cuts only |
| planet  | planet cuts only       |
| det     | detector cuts only     |
|---------+------------------------|

*** 28. What's the meaning of the different tags in the release file?
A release file may contain field like =tag_out=, =tag_cal=,
etc. Here's a summary of some common ones
- =tag_out=: Main cuts outputs. It contains both glitch cuts and
  detector cuts.
- =tag_cal=: Calibration outputs
- =tag_partial=: Cuts that only contain glitch cuts
- =tag_planet= / =tag_source=: Cuts that only contain planet/source cuts
- =tag_cmb=: cuts that contains glitch cuts, detector cuts, planet
  cuts and source cuts all merged.
Usually it's the firs two fields that are useful.
** Useful links
- cutparams repo: [[https://github.com/ACTCollaboration/cutparams]]
- cuts release page: https://github.com/ACTCollaboration/cutparams/releases
- TOD visualizer: [[https://github.com/guanyilun/tod_viz]]
- cuts validation repo: https://github.com/ACTCollaboration/cuts_validation
- Emacs plugins for cuts: [[https://github.com/guanyilun/cuts.el]]
- inputs for mapping: https://phy-wiki.princeton.edu/polwiki/pmwiki.php?n=Calibration.InputsForMapping
- ACT roundtable: [[https://actexperiment.info/roundtable]]
