* cutslib
Utility library for working with ACT cuts. As I get more familiar with
ACT cuts, this library may significantly change. Note that i have only
tested these on tiger.

Some notations that i used in these codes:
- Routines: main building blocks of cuts pipeline
- Modules: main building blocks of postprocessing pipeline
- Recipes: quick command line tools for various routine work

** Installation
#+BEGIN_SRC bash
pip install -e .
#+END_SRC

** Dependency
Most of the dependency libraries should automatically install, with a few
exceptions that require manual installation
- =moby2=
- =todloop=:
  #+BEGIN_SRC
  pip install git+https://github.com/guanyilun/todloop
  #+END_SRC

** Recipes
Located in =cuts.recipes=. Notable mentions (such that i don't forget
them myself):
- =cuts.recipes.results.combine=:
  reduce the output of mpi-based cuts pipeline of a given version (i.e. cutparams_v3.par):
  #+BEGIN_SRC bash
  cuts results combine cutparams_v3.par
  #+END_SRC
  This will find the corresponding run directory, combine mpi sub-tasks and remove redundent files
- =cuts.recipes.results.promote=:
  promote a given cuts parameter to a new version:
  #+BEGIN_SRC bash
  cuts results promote cutparams_v3.par
  #+END_SRC
  This will automatically replace of all the version numbers inside the file.
- =cuts.recipes.run.submit=:
  submit jobs (on tiger) for cuts pipeline with a given cutparam file (i.e. cutparam_v3.par)
  #+BEGIN_SRC bash
  cuts run submit cutparam_v3.par
  #+END_SRC
- =cuts.recipes.run.list=:
  list all cutparams in all tags and their corresponding slurm jobs, tod completion status, very useful
  #+BEGIN_SRC bash
  cuts run list
  #+END_SRC
  To see more than the latest version use =cuts run list all=
- =cuts.recipes.run.errors=:
  list all the errors generated during the run with a given cutparam file (i.e. cutparam_v3.par)
  #+BEGIN_SRC bash
  cuts run errors cutparam_v3.par
  #+END_SRC
  To see more than the latest version use =cuts run list all=
- =cuts.recipes.update.submit=:
  submit jobs (on tiger) for updating cut crits with a given cutparam file (i.e. cutparam_v3.par)
  #+BEGIN_SRC bash
  cuts update submit cutparam_v3.par
  #+END_SRC
- =cuts.recipes.dets.get_ff_unstable=:
  get unstable detectors from the flatfield file in a given cutparam file (i.e.cutparams_v3.par)
  #+BEGIN_SRC bash
  cuts dets get_ff_unstable cutparams_v3.par
  #+END_SRC
- =cuts.recipes.dets.get_exclude=:
  same as above but to get the excluded detectors
  #+BEGIN_SRC bash
  cuts dets get_exclude cutparams_v3.par
  #+END_SRC
- =cuts.recipes.dets.union=:
  get the union of det_uid found in each of the input file
  #+BEGIN_SRC bash
  cuts dets union exclude.dict lowgain.dict unstable.dict
  #+END_SRC
- =cuts.recipes.dets.intersection=:
  same as above but to find the intersection instead of the union
  #+BEGIN_SRC bash
  cuts dets intersection exclude.dict unstable.dict
  #+END_SRC
** Modules:
Notable mentions
- =collect_crit=:
  collect the cuts crit for all the TODs processed and generate a
  pickle file for easier processing
- =plot_cuts_thresholds=:
  take the pickle file generated by =collect_crit= and plot histograms
  of each of the cuts crit
- =get_flatfield=:
  generate atmospheric flatfield based on the atmosphere gain
- =plot_ff=:
  plot the flatfields (both planet and atmospheric)
- =plot_ff_binned=:
  same as above but for each pwv bin
- =plot_hits=:
  produce a hits map for the input TODs list
- =plot_array=:
  plot selected cuts crits (mean) on an array plot
- =plot_killed_array=:
  plot number of detectors alive after each cut crit
- =plot_ld_loading=:
  plot number of live detectors as a function of optical loading
- =plot_live_fraction=:
  plot the fraction that a detector is cut on an array plot
- =plot_planet_cal=:
  plot planet calibration as a function of optical loading
- =plot_resp_hist=:
  plot the histogram of number of detectors with valid responsivity
  from bias-step measurements.
- =plot_rms_gain=:
  produce a 2D histogram of responsivity versus planet flatfield gain
- =report=:
  generate a pdf report summarizing the latest run. It needs
  additional dependencies such as emacs and latex.
- =debug_patho=:
  pathology debugger (with ipdb inside)
- =export_json=:
  export the pathologies of each TOD into json format for easy visualization
  using this [[https://github.com/guanyilun/tod_viz][web-based tool]].
- =todlist_for_map=:
  generate the list of TODs that is available for mapping

** Q&A
*** 1. How is this different from moby2 cuts?
Most of the relevant scripts for cuts in moby2 have been migrated here. The purpose is
such that i can manage them easily without having to worry about compiling moby2, etc.
*** 2. What has been migrated from moby2 specifically?
- =moby2.analysis.tod_ana.pathologies= -> =cutslib.pathologies=
- =moby2.scripting.pathologies_tools= -> =cutslib.pathologies_tools=
- =moby2.analysis.tod_ana.visual= -> =cutslib.visual=
- =bin/{get,process}_cuts= -> various routines
*** 3. How to make =report.py= module work? What does it depend on externally?
It generates pdf report during the post-processing of the cuts results. I implemented
it using org format as it's more lucid than tex. The org document is converted to pdf
using emacs. This means that you will need to have two things available: 1. latex:
it is by default available on =tiger=, but it's lacking some of the libraries for the
pdf to compile properly so you will have to make these libraries available somehow.
The two libraries that i found missing are =ulem= and =wrapfig=. What i did was to
download these packages manually and place them in =~/texmf/tex/latex/ulem= and
=~/texmf/tex/latex/wrapfig= respectively. Emacs is not by default available on tiger
but you can easily load it as a module with =module load emacs= in your =.bashrc=.
Then you should have everything you need to get report generated
*** 4. How does recipe work?
Recipe as in my notation is simply a binding from command-line tool to a function in
the library. This is so that i can easily manage large number of cuts related command-line
tools by categories and have them easily accessible with the =cuts= keyword. These recipes
are defined in =cutslib.recipes=. An example collection of recipes is
=cutslib.recipes.results=. It contains some functions that help me manage the outputs from
cuts pipeline. For example, there is a function to merge mpi sub-task output into a single
one called =combine(cpar)=, where =cpar= refers to the path to a given cutparams file
(i.e. cutparams_v0.par). This function is made directly accessible in the command-line
via
#+BEGIN_SRC
cuts results combine cutparams_v0.par
#+END_SRC
Note that my convention is that each recipe function returns a list of commands to be
executed in sequence. It can be useful in some occasions.
*** 5. Environment variables, how are they used?
The environment variables can be used to define where the cuts depot is or where the
working directory of the cuts is, etc. The default values and the keys are described
in =cutslib.environ=. These can be set in the =.bashrc= with
#+BEGIN_SRC bash
export CUTS_DEPOT="/path/to/depot"
export CUTS_USER="somename"
export CUTS_PYENV="myenv"
#+END_SRC
Note that by default i am assuming you are running on a local environment given by =CUTS_PYENV=.
This is so that the python environment can be set properly before submitting slurm jobs.

*** 6. How did i exclude detectors with unstable flatfield and low gain detector?
As low gain detectors tend to result in low rms detectors which gets a
large weight in the map-making process, they are often identified
beforehand and excluded. Also, as the unstable detectors in the
flatfield are by default not removed in the cuts pipeline, they will
need to be excluded by hand. This question is to help me remember what
i need to do to get both of them excluded. First, to get the detectors
with unstable flatfield,
#+BEGIN_SRC bash
cuts dets get_ff_unstable cutparams_v3.par > unstable.dict
#+END_SRC
where cutparams_v3.par is an example cut parameter file of
interests. To get the detectors with low gain (from flatfield), what i
find easist is to run =export_json= module with =limit=1= option to
generate a json file associated with one of the TODs in the list, and
then pass this file to the TOD_viz visualizer (linked below).  In the
visualizer, i will manually en-circle the group of outliers with
abnormally low gain (in ff plot) using the interactive selection
tool. The det_uids of the selected detectors will be printed in the
browser console (pressing Ctrl+Shift+I to see it).  Suppose that i
store the resulting list of detectors into a file called
=lowgain.dict=. The last step is merge the =unstable.dict= and
=lowgain.dict= into the existing excluded list. I will do
#+BEGIN_SRC bash
cuts dets get_exclude cutparams_v3.par > exclude.dict
cuts dets union exclude.dict unstable.dict lowgain.dict > exclude_v2.dict
#+END_SRC
Then I will update the cutParam file to use =exclude_v2.dict= as the
excluded detector list.
*** 7. How to use =cuts.recipes.params.quickfix= to change some parameters?
Before i forget, this is how to perform a quick command-line fix of cutparams
#+BEGIN_SRC bash
cuts params quickfix exclude \"exclude_v2.dict\" cutParams_v3.par > cutParams_v4.par
#+END_SRC
Note the escaped string here. For non-string expression,
#+BEGIN_SRC bash
cuts params quickfix forceNoResp False cutParams_v3.par > cutParams_v4.par
#+END_SRC
To edit in place, add =-i= at the end of statement
#+BEGIN_SRC bash
cuts params quickfix forceNoResp False cutParams_v3.par -i
#+END_SRC
*** 8. What's my typical workflow when running cuts?
This is to document my workflow in case i forget them:
- switch to the right python environment (for my case =myenv=)
- launch emacs with =emacs -nw=
- launch the cuts-run plugin in emacs with =F7=
To edit an existing cut version:
- press =o= / =O= to open the cutparams/cutParams file for editing
To create a new version of cut:
- press =P= to promote the version of cut and edit like above
After editing:
- press =S= to submit the job to slurm
- press =L= to monitor the logs
When the job is done:
- press =[= to check the run folder
- press =E= to check the error logs to makes sure no unexpected errors occurred
- press =c= to reduce the mpi sub-tasks to a combined result
Post-processing
- press =f= to load up post-processing script
- select post-processing modules to run with =c-c m=
- press =F= to run the post-processing pipeline
- rsync the report to a local machine to view the reports

*** 8. How did i generate the lists of tods for preliminary studies?
From s17 onwards, we genenerate a list of 1000 TODs for preliminary studies before running on the full season. To generate the list of tods, i used the binary script =select_tod_adv= in the bin folder. It takes in a parameter file such as =tod.par= in the template folder. For example, in the bin folder,
#+BEGIN_SRC bash
./select_tod_adv ../templates/tod.par
#+END_SRC
The parameters in =tod.par= is explained in the commends in =select_tod_adv= script.

*** 9. How did i prepare the cuts h5 file for machine learning studies?
After the main cuts pipeline and post-processing pipeline finish, run the =generate_h5= post-processing module to create the h5 file for =mlpipe= to use.
The model generated from =mlpipe= can in turn be used to generate the det-level cuts using =model_to_cuts= post-processing module. Note that this is not the final version of cuts that can be used in production as it only contains the det-level results. To generate full results, one needs to run a modified cuts pipeline that will merge in all other cuts including partial cuts, etc. The modules required to do this is written in =cutslib.thirdparty= and the binary code that executes it is named =update_cuts= in bin folder.

*** 10. What's the main changes in the base-line cuts in s17 onwards compared with before?
1. calibration is purely based on planet flatfield and bias-step responsivity. No atmosphere gain is applied anymore. This is achieved by including the freq in the noAtmFreq in the cutParams file.
2. detectors without valid calibrations are no longer included. As we now calibrate by planet flatfield and bias-step responsivity, dets without flatfield or responsivity cannot be included reliably.
3. scan chunk level cuts are turned off by default as we no longer expect our detectors to behave drastically different in the duration of 10 mins.
4. pre-selection is no longer used to generate the cuts but instead used to calculate the common modes. The cuts are purely determined by the cuts thresholds, modulos manually exclusion.

*** 11. How does release work
Before i forget, this is how to generate a cut release. First go to the root cutparam directory. Suppose i want to release the latest cuts for s17, simply run
#+BEGIN_SRC bash
cuts release tags *s17*
#+END_SRC
It will promote me for a version name, author name and a file to write to. The default values should be reasonable for most of the cases. Here is a sample output, including the prompts:
#+begin_example
Version: (default: 20200328)
Author: (default: Yilun Guan)
{
  "version": "20200328",
  "date": "Mar 28, 2020",
  "tags": {
    "pa4_f150_s17_c11": {
      "params": "/projects/ACT/yilung/cuts/pa4_f150_s17_c11/cutparams_v6.par",
      "tag_out": "pa4_f150_s17_c11_v6",
      "tag_cal": "pa4_f150_s17_c11_v6",
      "tag_partial": "pa4_f150_s17_c11_v3_partial",
      "tag_planet": "pa4_f150_s17_c11_v3_planet",
      "tag_source": "pa4_f150_s17_c11_v3_source"
    },
    "pa4_f220_s17_c11": {
      "params": "/projects/ACT/yilung/cuts/pa4_f220_s17_c11/cutparams_v3.par",
      "tag_out": "pa4_f220_s17_c11_v3",
      "tag_cal": "pa4_f220_s17_c11_v3",
      "tag_partial": "pa4_f220_s17_c11_v2_partial",
      "tag_planet": "pa4_f220_s17_c11_v2_planet",
      "tag_source": "pa4_f220_s17_c11_v2_source"
    }
  },
  "author": "Yilun Guan",
  "depot": "/projects/ACT/yilung/depot/"
}
Write to: (default: /projects/ACT/yilung/depot/release_20200328.txt)
#+end_example
This is written in json format which should be easily digestible by modules in the downstream.
An example module that read the release is the =create_todinfo= module which generates the enki compatible =todinfo.txt= file.

*** 12. How to run final processing pipeline
What i call final processing pipeline is practically the same as the
normal post-processing pipeline except that it doesn't depend on a
specific cut version. It is supposed to depend on all cuts version. An
example of this is to create the =todinfo.hdf= file for map-making
which relies on information from all arrays / freqs pairs. An example
of such module is the =create_todinfo= module in
=cutslib.recipes.create_todinfo=. Here is an example final-processing
configuration script.
#+BEGIN_SRC
# Final Post-processing pipeline
# =====================================
# Full pipeline assuming all tag-specific processing is accomplished

[pipeline]
pipeline = create_todinfo

[create_todinfo]
cut_release = 20200327
obs_details_cmb = wide_01h_n
obs_details_noncmb = uranus
#+END_SRC
The only difference is that it doesn't depend on the cutparams, except
that everything should be the same as normal post-processing
pipelines.
** Useful links
- ACT roundtable: [[https://actexperiment.info/roundtable]]
- TOD visualizer: [[https://github.com/guanyilun/tod_viz]]
- Emacs plugins for cuts: [[https://github.com/guanyilun/cuts.el]]
- cutparams repo: [[https://github.com/guanyilun/cutparams]]
