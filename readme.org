* cutslib
Utility library for working with ACT cuts. As I get more familiar with
ACT cuts, this library may significantly change. Note that i have only
tested these on tiger.

Some notations that i used in these codes:
- Routines: main building blocks of cuts pipeline
- Modules: main building blocks of postprocessing pipeline
- Recipes: quick command line tools for various routine work

** Installation
#+BEGIN_SRC bash
pip install -e .
#+END_SRC

** Dependency
Most of the dependency libraries should automatically install, with a few
exceptions that require manual installation
- =moby2=
- =todloop=:
  #+BEGIN_SRC
  pip install git+https://github.com/guanyilun/todloop
  #+END_SRC

** Recipes
Located in =cuts.recipes=. Notable mentions (such that i don't forget
them myself):
- =cuts.recipes.results.combine=:
  reduce the output of mpi-based cuts pipeline of a given version (i.e. cutparams_v3.par):
  #+BEGIN_SRC bash
  cuts results combine cutparams_v3.par
  #+END_SRC
  This will find the corresponding run directory, combine mpi sub-tasks and remove redundent files
- =cuts.recipes.results.promote=:
  promote a given cuts parameter to a new version:
  #+BEGIN_SRC bash
  cuts results promote cutparams_v3.par
  #+END_SRC
  This will automatically replace of all the version numbers inside the file.
- =cuts.recipes.run.submit=:
  submit jobs (on tiger) for cuts pipeline with a given cutparam file (i.e. cutparam_v3.par)
  #+BEGIN_SRC bash
  cuts run submit cutparam_v3.par
  #+END_SRC
- =cuts.recipes.run.list=:
  list all cutparams in all tags and their corresponding slurm jobs, tod completion status, very useful
  #+BEGIN_SRC bash
  cuts run list
  #+END_SRC
  To see more than the latest version use =cuts run list all=
- =cuts.recipes.run.errors=:
  list all the errors generated during the run with a given cutparam file (i.e. cutparam_v3.par)
  #+BEGIN_SRC bash
  cuts run errors cutparam_v3.par
  #+END_SRC
- =cuts.recipes.update.submit=:
  submit jobs (on tiger) for updating cut crits with a given cutparam file (i.e. cutparam_v3.par)
  #+BEGIN_SRC bash
  cuts update submit cutparam_v3.par
  #+END_SRC
- =cuts.recipes.dets.get_ff_unstable=:
  get unstable detectors from the flatfield file in a given cutparam file (i.e.cutparams_v3.par)
  #+BEGIN_SRC bash
  cuts dets get_ff_unstable cutparams_v3.par
  #+END_SRC
- =cuts.recipes.dets.get_exclude=:
  same as above but to get the excluded detectors
  #+BEGIN_SRC bash
  cuts dets get_exclude cutparams_v3.par
  #+END_SRC
- =cuts.recipes.dets.union=:
  get the union of det_uid found in each of the input file
  #+BEGIN_SRC bash
  cuts dets union exclude.dict lowgain.dict unstable.dict
  #+END_SRC
- =cuts.recipes.dets.intersection=:
  same as above but to find the intersection instead of the union
  #+BEGIN_SRC bash
  cuts dets intersection exclude.dict unstable.dict
  #+END_SRC
** Modules:
Notable mentions
- =collect_crit=:
  collect the cuts crit for all the TODs processed and generate a
  pickle file for easier processing
- =plot_cuts_thresholds=:
  take the pickle file generated by =collect_crit= and plot histograms
  of each of the cuts crit
- =get_flatfield=:
  generate atmospheric flatfield based on the atmosphere gain
- =plot_ff=:
  plot the flatfields (both planet and atmospheric)
- =plot_ff_binned=:
  same as above but for each pwv bin
- =plot_hits=:
  produce a hits map for the input TODs list
- =plot_array=:
  plot selected cuts crits (mean) on an array plot
- =plot_killed_array=:
  plot number of detectors alive after each cut crit
- =plot_ld_loading=:
  plot number of live detectors as a function of optical loading
- =plot_live_fraction=:
  plot the fraction that a detector is cut on an array plot
- =plot_planet_cal=:
  plot planet calibration as a function of optical loading
- =plot_resp_hist=:
  plot the histogram of number of detectors with valid responsivity
  from bias-step measurements.
- =plot_rms_gain=:
  produce a 2D histogram of responsivity versus planet flatfield gain
- =report=:
  generate a pdf report summarizing the latest run. It needs
  additional dependencies such as emacs and latex.
- =debug_patho=:
  pathology debugger (with ipdb inside)
- =export_json=:
  export the pathologies of each TOD into json format for easy visualization
  using this [[https://github.com/guanyilun/tod_viz][web-based tool]].
- =todlist_for_map=:
  generate the list of TODs that is available for mapping

** Q&A
*** 1. How is this different from moby2 cuts?
Most of the relevant scripts for cuts in moby2 have been migrated here. The purpose is
such that i can manage them easily without having to worry about compiling moby2, etc.
*** 2. What has been migrated from moby2 specifically?
- =moby2.analysis.tod_ana.pathologies= -> =cutslib.pathologies=
- =moby2.scripting.pathologies_tools= -> =cutslib.pathologies_tools=
- =moby2.analysis.tod_ana.visual= -> =cutslib.visual=
- =bin/{get,process}_cuts= -> various routines
*** 3. How to make =report.py= module work? What does it depend on externally?
It generates pdf report during the post-processing of the cuts results. I implemented
it using org format as it's more lucid than tex. The org document is converted to pdf
using emacs. This means that you will need to have two things available: 1. latex:
it is by default available on =tiger=, but it's lacking some of the libraries for the
pdf to compile properly so you will have to make these libraries available somehow.
The two libraries that i found missing are =ulem= and =wrapfig=. What i did was to
download these packages manually and place them in =~/texmf/tex/latex/ulem= and
=~/texmf/tex/latex/wrapfig= respectively. Emacs is not by default available on tiger
but you can easily load it as a module with =module load emacs= in your =.bashrc=.
Then you should have everything you need to get report generated
*** 4. How does recipe work?
Recipe as in my notation is simply a binding from command-line tool to a function in
the library. This is so that i can easily manage large number of cuts related command-line
tools by categories and have them easily accessible with the =cuts= keyword. These recipes
are defined in =cutslib.recipes=. An example collection of recipes is
=cutslib.recipes.results=. It contains some functions that help me manage the outputs from
cuts pipeline. For example, there is a function to merge mpi sub-task output into a single
one called =combine(cpar)=, where =cpar= refers to the path to a given cutparams file
(i.e. cutparams_v0.par). This function is made directly accessible in the command-line
via
#+BEGIN_SRC
cuts results combine cutparams_v0.par
#+END_SRC
Note that my convention is that each recipe function returns a list of commands to be
executed in sequence. It can be useful in some occasions.
*** 5. Environment variables, how are they used?
The environment variables can be used to define where the cuts depot is or where the
working directory of the cuts is, etc. The default values and the keys are described
in =cutslib.environ=. These can be set in the =.bashrc=. Here is how i setup
the environment variables
#+BEGIN_SRC bash
export CUTS_DEPOT="/projects/ACT/yilung/depot/"
export CUTS_DIR="/projects/ACT/yilung/cuts/"
export CUTS_PYENV="myenv"
export CUTS_SHARED_DEPOT="/projects/ACT/yilung/actpol_data_shared/"
export CUTS_MAPDATA="/scratch/gpfs/yilung/mapdata"
#+END_SRC
Note that by default i am assuming you are running on a local
environment given by =CUTS_PYENV=.  This is so that the python
environment can be set properly before submitting slurm jobs.  To
furthur elaborate, =CUTS_DEPOT= is where the outputs of the cuts
are. =CUTS_DIR= is where the working directory of the cuts are. This
is such that various recipes such as =cuts run list= can find the
parameter files that i am working on, it's a pre-requisite for the
emacs cut plug-in. =CUTS_PYENV= is the local python environment as
described earlier. =CUTS_SHARED_DEPOT= is the path to shared depot,
and =CUTS_MAPDATA= is the path to the directory to work with mapdata.
This is used by the =build_todinfo.py= module.
*** 6. How did i exclude detectors with unstable flatfield and low gain detector?
As low gain detectors tend to result in low rms detectors which gets a
large weight in the map-making process, they are often identified
beforehand and excluded. Also, as the unstable detectors in the
flatfield are by default not removed in the cuts pipeline, they will
need to be excluded by hand. This question is to help me remember what
i need to do to get both of them excluded. First, to get the detectors
with unstable flatfield,
#+BEGIN_SRC bash
cuts dets get_ff_unstable cutparams_v3.par > unstable.dict
#+END_SRC
where cutparams_v3.par is an example cut parameter file of
interests. To get the detectors with low gain (from flatfield), what i
find easist is to run =export_json= module with =limit=1= option to
generate a json file associated with one of the TODs in the list, and
then pass this file to the TOD_viz visualizer (linked below).  In the
visualizer, i will manually en-circle the group of outliers with
abnormally low gain (in ff plot) using the interactive selection
tool. The det_uids of the selected detectors will be printed in the
browser console (pressing Ctrl+Shift+I to see it).  Suppose that i
store the resulting list of detectors into a file called
=lowgain.dict=. The last step is merge the =unstable.dict= and
=lowgain.dict= into the existing excluded list. I will do
#+BEGIN_SRC bash
cuts dets get_exclude cutparams_v3.par > exclude.dict
cuts dets union exclude.dict unstable.dict lowgain.dict > exclude_v2.dict
#+END_SRC
Then I will update the cutParam file to use =exclude_v2.dict= as the
excluded detector list. Note that if this is the only change, one does
not need to re-run cuts pipeline entirely because exclude detector list
is applied in the very end. One can remove the db file
generated in the run_* folder and re-run. This will skip the glitch,
planet, source cuts and multi-frequency analysis which will save
considerable amount of time. One can also run
#+BEGIN_SRC bash
cuts submit update_crit cutparams_v0.par
#+END_SRC
which will be even faster.
*** 7. How to use =cuts.recipes.params.quickfix= to change some parameters?
Before i forget, this is how to perform a quick command-line fix of cutparams
#+BEGIN_SRC bash
cuts params quickfix exclude \"exclude_v2.dict\" cutParams_v3.par > cutParams_v4.par
#+END_SRC
Note the escaped string here. For non-string expression,
#+BEGIN_SRC bash
cuts params quickfix forceNoResp False cutParams_v3.par > cutParams_v4.par
#+END_SRC
To edit in place, add =-i= at the end of statement
#+BEGIN_SRC bash
cuts params quickfix forceNoResp False cutParams_v3.par -i
#+END_SRC
*** 8. What's my typical workflow when running cuts?
This is to document my workflow in case i forget them:
- switch to the right python environment (for my case =myenv=)
- launch emacs with =emacs -nw=
- launch the cuts-run plugin in emacs with =F7=
To edit an existing cut version:
- press =o= / =O= to open the cutparams/cutParams file for editing
To create a new version of cut:
- press =P= to promote the version of cut and edit like above
After editing:
- press =S= to submit the job to slurm
- press =L= to monitor the logs
When the job is done:
- press =[= to check the run folder
- press =E= to check the error logs to makes sure no unexpected errors occurred
- press =c= to reduce the mpi sub-tasks to a combined result
Post-processing
- press =f= to load up post-processing script
- select post-processing modules to run with =c-c m=
- press =F= to run the post-processing pipeline
- rsync the report to a local machine to view the reports

*** 8. How did i generate the lists of tods for preliminary studies?
From s17 onwards, we genenerate a list of 1000 TODs for preliminary studies before running on the full season. To generate the list of tods, i used the binary script =select_tod_adv= in the bin folder. It takes in a parameter file such as =tod.par= in the template folder. For example, in the bin folder,
#+BEGIN_SRC bash
./select_tod_adv ../templates/tod.par
#+END_SRC
The parameters in =tod.par= is explained in the commends in =select_tod_adv= script.

*** 9. How did i prepare the cuts h5 file for machine learning studies?
After the main cuts pipeline and post-processing pipeline finish, run
the =generate_h5= post-processing module to create the h5 file for
=mlpipe= to use.  The model generated from =mlpipe= can in turn be
used to generate the det-level cuts using =model_to_cuts=
post-processing module. Note that this is not the final version of
cuts that can be used in production as it only contains the det-level
results. To generate full results, one needs to run a modified cuts
pipeline that will merge in all other cuts including partial cuts,
etc. The modules required to do this is written in
=cutslib.thirdparty= and the binary code that executes it is named
=update_cuts= in bin folder.

*** 10. What's the main changes in the base-line cuts in s17 onwards compared with before?
1. calibration is purely based on planet flatfield and bias-step responsivity. No atmosphere gain is applied anymore. This is achieved by including the freq in the noAtmFreq in the cutParams file.
2. detectors without valid calibrations are no longer included. As we now calibrate by planet flatfield and bias-step responsivity, dets without flatfield or responsivity cannot be included reliably.
3. scan chunk level cuts are turned off by default as we no longer expect our detectors to behave drastically different in the duration of 10 mins.
4. pre-selection is no longer used to generate the cuts but instead used to calculate the common modes. The cuts are purely determined by the cuts thresholds, modulos manually exclusion.

*** 11. How does release work
Before i forget, this is how to generate a cut release. First go to the root cutparam directory. Suppose i want to release the latest cuts for s17, simply run
#+BEGIN_SRC bash
cuts release tags *s17*
#+END_SRC
It will promote me for a version name, author name and a file to write to. The default values should be reasonable for most of the cases. Here is a sample output, including the prompts:
#+begin_example
Version: (default: 20200328)
Author: (default: Yilun Guan)
{
  "version": "20200328",
  "date": "Mar 28, 2020",
  "tags": {
    "pa4_f150_s17_c11": {
      "params": "/projects/ACT/yilung/cuts/pa4_f150_s17_c11/cutparams_v6.par",
      "tag_out": "pa4_f150_s17_c11_v6",
      "tag_cal": "pa4_f150_s17_c11_v6",
      "tag_partial": "pa4_f150_s17_c11_v3_partial",
      "tag_planet": "pa4_f150_s17_c11_v3_planet",
      "tag_source": "pa4_f150_s17_c11_v3_source"
    },
    "pa4_f220_s17_c11": {
      "params": "/projects/ACT/yilung/cuts/pa4_f220_s17_c11/cutparams_v3.par",
      "tag_out": "pa4_f220_s17_c11_v3",
      "tag_cal": "pa4_f220_s17_c11_v3",
      "tag_partial": "pa4_f220_s17_c11_v2_partial",
      "tag_planet": "pa4_f220_s17_c11_v2_planet",
      "tag_source": "pa4_f220_s17_c11_v2_source"
    }
  },
  "author": "Yilun Guan",
  "depot": "/projects/ACT/yilung/depot/"
}
Write to: (default: /projects/ACT/yilung/depot/release_20200328.txt)
#+end_example
This is written in json format which should be easily digestible by
modules in the downstream.  An example module that read the release is
the =create_todinfo= module which generates the enki compatible
=todinfo.txt= file. After the release json file is generated, tag
the =cutparam= repo with the same tag to keep track of the parameters
used in the given tag.

*** 12. How to run final processing pipeline
What i call final processing pipeline is practically the same as the
normal post-processing pipeline except that it doesn't depend on a
specific cut version. It is supposed to depend on all cuts version. An
example of this is to create the =todinfo.hdf= file for map-making
which relies on information from all arrays / freqs pairs. An example
of such module is the =create_todinfo= module in
=cutslib.recipes.create_todinfo=. Here is an example final-processing
configuration script.
#+BEGIN_SRC
# Final Post-processing pipeline
# =====================================
# Full pipeline assuming all tag-specific processing is accomplished

[pipeline]
pipeline = create_todinfo

[create_todinfo]
cut_release = 20200327
obs_details_cmb = wide_01h_n
obs_details_noncmb = uranus
#+END_SRC
The only difference is that it doesn't depend on the cutparams, except
that everything should be the same as normal post-processing
pipelines.
*** 13. How to setup environment for enki
To setup environment to run enki or enki related modules do
#+BEGIN_SRC
module load enki
module load so_stack
#+END_SRC
on tiger.
*** 14. What are different types of modules?
At the moment, the post-processing pipeline (=cutspipe=) supports
different types of modules. By default the module is an =internal=
type which means it will be loaded from =cutslib.modules= by
default. The default name is the pipeline module name unless specified
otherwise by =module= option. It also supports external module that
can be loaded from a given file. To use this one needs to specify
=type= option and =file= option to tell the pipeline where to look for
the module. Here is an example configuration for an external module
#+BEGIN_SRC
[create_todinfo]
type = external
file = /home/yilung/work/cutslib/cutslib/modules/create_todinfo.py

cut_release = 20200327
obs_details_cmb = wide_01h_n
obs_details_noncmb = uranus
outfile = /scratch/gpfs/yilung/mapdata/s17_subset/todinfo.txt
#+END_SRC
Another type of module that is supported is the =command= type. It
is essentially like running command in bash, here is an example
#+BEGIN_SRC
[test_module]
type = command
dir = pa4_f220_s18_c11
command = ls ${dir}
#+END_SRC
Note that we have enabled and used the =ExtendedInterpolation= syntax
for variable substitution. Refer to =ConfigParser= for more
documentation on how to use this.
*** 15. How to use =cutspipe= modules with mpi?
The post-processing pipeline =cutspipe= is made to support mpi. In
order for a module to make use of mpi, it needs to be supported in the
module explicitly. The relevant dependencies are injected through the
argument to the =run(p)= method call with the rank, size and comm
contained in =p.rank=, =p.size=, =p.comm=. If a module support mpi, it
can declared in the option with =mpi=True=. This allows the main
program to pass mpi control to the module instead of forcing it to run
with =rank=0= for other modules that do not support mpi. See
=cutslib.modules.collect_crit= module for an example of how mpi is
supported.
*** 16. How do i remove tod runs specific errors from running next time?
Sometimes errors are unavoidable. For example, preselection errors are unavoidable
sometimes and this happens at the very end of the pipeline so if we know for sure a
tod is going to fail pre-selection, we shoudn't have spent long time to run the previous
routines. One way out of this is to include these tods in a skip list. For example,
here is how i get the tods with an error message containing "Presel" (preselection errors)
and pipe the results to a skip list. Then i can put this list in the cutparams file
in the reject_depot list.
#+BEGIN_SRC bash
cuts run errors_tod cutparams_v0.par presel > skip_tods.txt
#+END_SRC
*** 17. How to add or recover detector cuts for different TODs?
In the cutParams file that is an option called =include_cuts=, which belongs to
=pathologyParams= section and look like
#+BEGIN_SRC python
     'include_cuts'         : [
                            {
                               "type": "exclude",
                               "file": "/projects/ACT/yilung/cuts/pa4_f150_s17_c11/cuts_de_ge500_norm_ge2e4_rms_le1.5.hdf",
                            },
                            {
                               "type": "exclude",
                               "file": "/projects/ACT/yilung/cuts/pa4_f150_s17_c11/cuts_gain_le0.2_rms_le1.5.hdf",
                            },
     ]
#+END_SRC
There are two types of operations supported at this moment: =exclude=
and =recover=. =exclude= means to remove dets from some TODs, and
=recover= means to prevent these list of dets being cut for some
TODs. A good use case of this is to recover some of the detectors cut
in low pwv TODs. The file format supported is an hdf file that is
generated with the name of tod as key and a boolean mask representing
dets selection. This file can be easily generated using =cutslib.season.SeasonStats=
object.
*** 18. Quickly update crit or dets cuts without rerunning =findPathologies=
I have mentioned using =cuts submit update_crit= to update the cuts criteria
without rerunning the entire pathology finding pipeline. This is useful when
one wants to submit jobs to slurm and don't mind the queuing process, if a quick
look is needed, there is a module doing similar thing in =cutslib.modules.update_cuts=.
It is mpi-enabled so running through 1000 TODs takes about a minute on one node.
One just need to add to the configuration file
#+BEGIN_SRC
[update_cuts]
mpi=True
#+END_SRC
and include in the =pipeline= section. Run it like =mpirun -n 40 cutspipe post.ini=.
It's good to remove the previous TODCuts, calibration from depot, though
i haven't encountered a case where not doing so causes noticable issues. It's good
to run this in a debug note because this is very quick to run, here is the command
to launch a debug node with 40 tasks required
#+BEGIN_SRC
salloc -pdebug --nodes 1 --ntasks-per-node=40 --time=1:00:00
#+END_SRC
*** 19. How to setup enki for a given cut release?
Here is what i do:

- create an =.enkirc= in the home directory with, for example,
#+BEGIN_SRC
root = '/scratch/gpfs/yilung/mapdata'
#+END_SRC
- make sure environment variable =CUTS_MAPDATA= points to the same path specified
in =.enkirc=. Also specify the path to the cuts depot in =CUTS_DEPOT= environment
variable. For example,
#+BEGIN_SRC bash
export CUTS_DEPOT="/projects/ACT/yilung/depot/"
export CUTS_MAPDATA="/scratch/gpfs/yilung/mapdata"
#+END_SRC
- Build metadata for a given release. Use the following cutspipe modules
#+BEGIN_SRC
[create_todinfo]
cut_release = 20200327
obs_details_cmb = wide_01h_n
obs_details_noncmb = uranus
dataset = s17_subset
outfile = ${cuts_mapdata}/${dataset}/todinfo.txt

[create_filedb]
cut_release = 20200327
dataset = s17_subset
template = default
outfile = ${cuts_mapdata}/${dataset}/filedb.txt

[build_todinfo]
n_tasks = 10
dataset = s17_subset
sel = s17
#+END_SRC
The fields should be self explanary. These modules produce =todinfo.txt=, =filedb.txt=
and =todinfo.hdf= based on a given =cut_release= tag. with this we are all set to run
enki. Some of existing mapping routines in cutslib include =map_tods= which produces
binned map of TODs, and =tod2map2= which produce maximum likelihood code using =tod2map2=
in =tenki=. here are the default configurations for them, one only needs to overwrite
relevant fields when running.
#+BEGIN_SRC
[map_tods]
type=command
ntasks=10
module_load=enki so_stack
area=wide_01h_n
dataset=s17_subset
sel=s17,cmb
tag=test
nrandom=100
mapdata=${cuts_mapdata}
command=mpirun -n {ntasks} python map_tod.py ${mapdata}/area/${area}.fits \
        "${sel}" ${CUTS_MAPDATA}/{dataset}/{tag} --dataset ${dataset} \
        --nrandom ${nrandom}

[tod2map2]
type = command
module_load = enki so_stack
ntasks = 4
area = wide_01h_n
sel = s17,cmb
dataset = s17_subset
tag = test
downsample = 4
verbosity = 2
nomp = 4
mapdata = ${cuts_mapdata}
command = srun -n ${ntasks} enki tod2map2 -S sky:${area} "${sel}" ${mapdata}/${tag} --verbosity=${verbosity} --downsample=${downsample} --dataset ${dataset}
#+END_SRC
more relevant mapping script written by Adri that's relevant for cuts
can be found in cuts_validation repo linked in the useful link section.
*** 20. Why is gain calculation in moby2 so confusing?
The manipulation of gains in moby2 / loic's codes are beyond
confusing. I kept getting confused on the calculation and every time
needing to spend lots of time recapping what's done. Let me write down
what i understand now before i forget again...
- gains are calculated with SVD using calibrated TODs (with input
  flatfield and resp) as part of =lowFreqAnal= [[https://github.com/ACTCollaboration/moby2/blob/master/python/analysis/tod_ana/pathologies.py#L1883][here]] for each frequency
  window.
- the gains for each frequency window are then normalized by mean of a
  set of "reliable" detectors. This set of dets is determined by the
  preselection, that is, dets that are preselected in more than half
  of the frequency windows. See [[https://github.com/ACTCollaboration/moby2/blob/master/python/analysis/tod_ana/pathologies.py#L1691][this line]].
- estimate detector gains using the mean of the gains measured from
  different frequency windows, this is similar to how other low
  frequency pathological parameters are derived. See [[https://github.com/ACTCollaboration/moby2/blob/master/python/analysis/tod_ana/pathologies.py#L1700][this line]].
- The gains estimated above is then divided by the input flatfield to
  "remove" the effects of flatfield. In the case that the input
  flatfield 100% accurate for a TOD, the gain without removing
  flatfield will be exactly 1, so after removing the flatfield the
  gain will be 1/ff which matches with the gains from the input
  flatfield. See [[https://github.com/ACTCollaboration/moby2/blob/master/python/analysis/tod_ana/pathologies.py#L416][this line]].
- This gains will be stored in the crit field in the pathologies
  object and saved to disk.
- When cuts are to be generated based on the pathologies object, that
  is, in the =recoverScanCuts= method of a =reportPathologies= object,
  it calls the =calibrateValues= method in the =makeNewSelections=
  method of pathology object. Here the flatfield effects are added
  back again to the gain (=gain <- gain * ff=). See [[https://github.com/ACTCollaboration/moby2/blob/master/python/analysis/tod_ana/pathologies.py#L695][this line]]. This
  gain is normalized once again in the =normalizeGains= function by
  the mean of the gain from a good subset of the detectors which is
  found after outlier removal. See [[https://github.com/ACTCollaboration/moby2/blob/master/python/analysis/tod_ana/pathologies.py#L2266][this line]]. It is this set of gains
  that the cut criteria specified in the cut parameter scripts apply
  to.
- On the other hand, the gain stored in the pathology object is later
  collected into a pickle file by =collectSeasonCrit= script and
  stored under =gainLive= in the pickle file. Note that these gains
  have flatfield removed. See [[https://github.com/ACTCollaboration/moby2/blob/master/bin/collectSeasonCrit][this script]]. Atmosphere flatfields are
  generated based on these =gainLive= field as =mean(1/gain)= using 
  a subset of dets after outlier rejection. See [[https://github.com/ACTCollaboration/moby2/blob/master/bin/generateFlatfield#L32][this line]]. These gains
  are normalized again by the mean of this subset of dets which is also 
  labelled as stable. This process of taking =mean(1/gain)= and normalization
  is repeated for a total of *10 times*... The results of which, after some
  threshold-based cuts become the output atmosphere flatfield.

This is the life story of the gains in =moby2=.
  
*** 21. How to pre-process the bias-step measurements such that they match with TODs?
BS measurements provided by Patty is aranged in ctimes, i will first
need to match them to TODs. To do this matching for a given season and
array, first make sure that the bias-step files from Patty is rsync-ed
to the depot with an example structure like this (for pa4), notice
this should be the same as the file-structure used by Patty.
#+begin_example
/home/yilung/work/depot/biasstep/2019/calibration_20200328/pa4
#+end_example
After re-assuring that the bias-steps files are rsync-ed completely,
one can then use the module =match_bs_tod= to match them to tods with
a configuration file like this:
#+BEGIN_SRC 
[pipeline]
cutparam = cutparams_v0.par
output_dir = /projects/ACT/yilung/depot/Postprocess/
pipeline = match_bs_tod
[match_bs_tod]
tag = calibration_20200328
#+END_SRC
i store it in =pre.ini= and run =cutspipe pre.ini= to complete the
step. It should take one or two minutes to complete the matching. Note
that this only needs to be done once per array. When this is done, one
can supply it to the cutParams config with something like
#+begin_example
'config'   : [
{
   "type": "depot_cal",
   "tag": "pa6_s18_bs_calibration_20200325",
   "depot": '/projects/ACT/yilung/depot',
   "name": "biasstep"
}],
#+end_example
Substitute the season, array and bs tag accordingly. 
** Useful links
- inputs for mapping: https://phy-wiki.princeton.edu/polwiki/pmwiki.php?n=Calibration.InputsForMapping
- cutparams repo: [[https://github.com/guanyilun/cutparams]]
- cuts validation repo: https://github.com/ACTCollaboration/cuts_validation
- ACT roundtable: [[https://actexperiment.info/roundtable]]
- TOD visualizer: [[https://github.com/guanyilun/tod_viz]]
- Emacs plugins for cuts: [[https://github.com/guanyilun/cuts.el]]
